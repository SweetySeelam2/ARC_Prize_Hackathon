{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91496,"databundleVersionId":11802066,"sourceType":"competition"},{"sourceId":13102453,"sourceType":"datasetVersion","datasetId":8299790}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ARC Prize 2025 – ULTIMATE RECURSIVE MULTI-MODAL ARCHITECTURE\n\n# Cell 1: Setup & Paths\nimport os, json, itertools, math, time, random\nfrom collections import Counter, defaultdict, deque\nfrom copy import deepcopy\nimport numpy as np\n\n# Determinism\nrandom.seed(42)\nnp.random.seed(42)\n\n# Kaggle paths\nTRAIN_JSON = \"/kaggle/input/arc-prize-2025/arc-agi_training_challenges.json\"\nTEST_JSON  = \"/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json\"\nOUTPUT_DIR = \"/kaggle/working\"\nSUBMISSION_PATH = os.path.join(OUTPUT_DIR, \"submission.json\")\n\n# Runtime knobs\nMAX_TIME_PER_TASK = 3.0    # seconds per task\nUSE_COMPOSITIONS = True    # enable depth-2 compositions\nPRINT_PROGRESS_EVERY = 50\n\n# Operation categorization for enhanced diversity ranking (Cell 5)\nOP_CATEGORIES = {\n    'identity': ['op_identity'],\n    'rotation': ['op_rotate90', 'op_rotate180', 'op_rotate270'],\n    'reflection': ['op_mirror_h', 'op_mirror_v', 'op_transpose'],\n    'cropping': ['op_crop_center_pad', 'op_largest_cc_center', 'op_trim_to_non_background_grid'],\n    'recolor': ['op_recolor_learned', 'op_invert_colors', 'op_normalize_color_to_1', 'op_recolor_by_ratio'],\n    'structural': ['op_majority_fill', 'op_tile_smallest_patch', 'op_fill_holes', 'op_complete_by_symmetry'],\n    'cc_manip': ['op_keep_largest_cc_only', 'op_grow_largest_cc', 'op_shrink_largest_cc', 'op_replace_cc_with_pixel', 'op_process_blocks'],\n    'io_abstraction': ['op_io_global_color_map'],\n    'object_manip': ['op_recursive_cc_transformation'],  # NEW SOLVER C\n}\ndef get_op_category(lbl):\n    name = lbl.split(':')[-1].split('+')[0]\n    for cat, ops in OP_CATEGORIES.items():\n        if name in ops: return cat\n    return 'other'\n\n# Cell 2: Robust ARC Task Loader\ndef _is_task(obj):\n    return isinstance(obj, dict) and (\"train\" in obj) and (\"test\" in obj)\n\ndef _looks_like_task_dict(d):\n    if not isinstance(d, dict) or not d:\n        return False\n    sample_vals = list(d.values())[:5]\n    return all(_is_task(v) for v in sample_vals if isinstance(v, dict))\n\ndef _unwrap_wrappers(obj):\n    if isinstance(obj, dict):\n        for key in [\"tasks\", \"task_list\", \"training\", \"train_tasks\", \"data\", \"items\"]:\n            if key in obj and isinstance(obj[key], list):\n                return obj[key]\n    return obj\n\ndef load_arc_tasks_anyshape(path):\n    with open(path, \"r\") as f:\n        raw = f.read().strip()\n\n    if not (raw.startswith(\"{\") or raw.startswith(\"[\")):\n        out = []\n        for line in raw.splitlines():\n            s = line.strip()\n            if not s:\n                continue\n            o = json.loads(s)\n            if _is_task(o):\n                out.append(o)\n            else:\n                o = _unwrap_wrappers(o)\n                if isinstance(o, list) and o and _is_task(o[0]):\n                    out.extend(o)\n                elif isinstance(o, dict) and _looks_like_task_dict(o):\n                    out.extend(list(o.values()))\n        if not out:\n            raise ValueError(\"Could not parse JSONL into ARC tasks.\")\n        return out\n\n    obj = json.loads(raw)\n    if _is_task(obj):\n        return [obj]\n    obj = _unwrap_wrappers(obj)\n    if isinstance(obj, list):\n        if len(obj) == 1 and not _is_task(obj[0]):\n            inner = _unwrap_wrappers(obj[0])\n            if isinstance(inner, list) and inner and _is_task(inner[0]):\n                return inner\n        if obj and _is_task(obj[0]):\n            return obj\n        merged = []\n        for it in obj:\n            if _is_task(it):\n                merged.append(it)\n            elif isinstance(it, dict) and _looks_like_task_dict(it):\n                merged.extend(list(it.values()))\n        if merged:\n            return merged\n\n    if isinstance(obj, dict) and _looks_like_task_dict(obj):\n        return list(obj.values())\n\n    raise ValueError(f\"Unrecognized ARC format for {path}\")\n\n# Cell 3: Core grid utilities\ndef to_np(grid): return np.array(grid, dtype=int)\ndef to_list(arr): return arr.tolist()\ndef bg_color(arr):\n    vals, cnts = np.unique(arr, return_counts=True)\n    return int(vals[np.argmax(cnts)]) if len(vals)>0 else 0\ndef hamming(a,b):\n    if a.shape != b.shape: return 10**9\n    return int(np.sum(a!=b))\ndef rotate(arr,k=1): return np.rot90(arr,k=k)\ndef mirror_h(arr): return arr[:, ::-1]\ndef mirror_v(arr): return arr[::-1, :]\ndef transpose(arr): return arr.T\ndef crop_to_bbox(arr, bg=None):\n    if bg is None: bg = bg_color(arr)\n    ys, xs = np.where(arr != bg)\n    if len(ys)==0: return arr.copy()\n    y0,y1 = ys.min(), ys.max()\n    x0,x1 = xs.min(), xs.max()\n    return arr[y0:y1+1, x0:x1+1]\ndef bbox_of_pixels(pixels):\n    if not pixels: return 0,0,0,0\n    ys=[p[0] for p in pixels]; xs=[p[1] for p in pixels]\n    return min(ys), min(xs), max(ys), max(xs)\n\ndef pad_to(arr, H, W, bg=None, align='center'):\n    if bg is None: bg = bg_color(arr)\n    h, w = arr.shape\n    out = np.full((H, W), bg, dtype=int)\n    if align == 'center':\n        dy = (H - h) // 2; dx = (W - w) // 2\n    else:  # top-left\n        dy = 0; dx = 0\n    y0 = max(0, dy); x0 = max(0, dx)\n    sy = max(0, -dy); sx = max(0, -dx)\n    copy_h = min(H - y0, h - sy)\n    copy_w = min(W - x0, w - sx)\n    if copy_h > 0 and copy_w > 0:\n        out[y0:y0+copy_h, x0:x0+copy_w] = arr[sy:sy+copy_h, sx:sx+copy_w]\n    return out\n\ndef connected_components(arr, bg=None):\n    if bg is None: bg = bg_color(arr)\n    H,W = arr.shape\n    seen = np.zeros((H,W), dtype=bool)\n    comps=[]\n    for y in range(H):\n        for x in range(W):\n            if seen[y,x] or arr[y,x]==bg: continue\n            color = arr[y,x]\n            stack=deque([(y,x)]); seen[y,x]=True; pix=[]\n            while stack:\n                cy,cx = stack.popleft()\n                pix.append((cy,cx))\n                for dy,dx in [(1,0),(-1,0),(0,1),(0,-1)]:\n                    ny,nx = cy+dy, cx+dx\n                    if 0<=ny<H and 0<=nx<W and not seen[ny,nx] and arr[ny,nx]==color:\n                        seen[ny,nx]=True; stack.append((ny,nx))\n            comps.append((color,pix))\n    return comps\n\ndef majority_color(arr):\n    vals, cnts = np.unique(arr, return_counts=True)\n    return int(vals[np.argmax(cnts)]) if len(vals)>0 else 0\n\ndef resize_integer_scale(obj_arr, target_h, target_w, bg=None):\n    if bg is None: bg = bg_color(obj_arr)\n    h,w = obj_arr.shape\n    if h==0 or w==0:\n        return np.full((target_h,target_w), bg, dtype=int)\n    sy = max(1, round(target_h / h))\n    sx = max(1, round(target_w / w))\n    scaled = np.repeat(np.repeat(obj_arr, sy, axis=0), sx, axis=1)\n    sh,sw = scaled.shape\n    if sh>=target_h and sw>=target_w:\n        y0=(sh-target_h)//2; x0=(sw-target_w)//2\n        scaled = scaled[y0:y0+target_h, x0:x0+sw]\n    else:\n        scaled = pad_to(scaled, target_h, target_w, bg=bg, align='center')\n    return scaled\n\ndef smallest_tile(arr):\n    H,W = arr.shape\n    for th in range(1,H+1):\n        if H%th: continue\n        for tw in range(1,W+1):\n            if W%tw: continue\n            tile = arr[0:th,0:tw]\n            if np.array_equal(np.tile(tile,(H//th, W//tw)), arr):\n                return tile\n    return None\n\ndef dilate(arr, bg=None, k=1):\n    if bg is None: bg = bg_color(arr)\n    H, W = arr.shape\n    out = arr.copy()\n    for _ in range(k):\n        next_out = out.copy()\n        ys, xs = np.where(out != bg)\n        for y, x in zip(ys, xs):\n            color = out[y, x]\n            for dy, dx in [(0,1), (0,-1), (1,0), (-1,0)]:\n                ny, nx = y + dy, x + dx\n                if 0 <= ny < H and 0 <= nx < W and out[ny, nx] == bg:\n                    next_out[ny, nx] = color\n        out = next_out\n    return out\n\ndef erode(arr, bg=None, k=1):\n    if bg is None: bg = bg_color(arr)\n    H, W = arr.shape\n    out = arr.copy()\n    for _ in range(k):\n        next_out = out.copy()\n        for y in range(H):\n            for x in range(W):\n                if out[y, x] != bg:\n                    is_edge = False\n                    for dy, dx in [(0,1), (0,-1), (1,0), (-1,0)]:\n                        ny, nx = y + dy, x + dx\n                        if not (0 <= ny < H and 0 <= nx < W) or out[ny, nx] == bg:\n                            is_edge = True\n                            break\n                    if is_edge:\n                        next_out[y, x] = bg\n        out = next_out\n    return out\n\ndef decompose_into_blocks(arr, target_h, target_w):\n    H, W = arr.shape\n    blocks = []\n\n    if H % target_h == 0 and W % target_w == 0:\n        for r in range(0, H, target_h):\n            for c in range(0, W, target_w):\n                blocks.append(arr[r:r+target_h, c:c+target_w])\n        return blocks\n\n    if H > 1 and W > 1:\n        for th in range(1, min(H, 5)):\n            if H % th == 0:\n                for tw in range(1, min(W, 5)):\n                    if W % tw == 0:\n                        blocks = []\n                        for r in range(0, H, th):\n                            for c in range(0, W, tw):\n                                blocks.append(arr[r:r+th, c:c+tw])\n                        return blocks\n    return [arr]\n\n# Cell 4: Heuristic operations (ULTIMATE FINAL UPGRADE - Recursive CC Solver Added)\ndef learn_color_map(train_pairs):\n    mapping={}\n    for xin,xout in train_pairs:\n        cin=Counter(xin.flatten()); cout=Counter(xout.flatten())\n        if not cin or not cout: continue\n        bg_in=max(cin,key=cin.get); bg_out=max(cout,key=cout.get)\n        in_sorted =[c for c,_ in cin.most_common() if c!=bg_in]\n        out_sorted=[c for c,_ in cout.most_common() if c!=bg_out]\n        for i,c in enumerate(in_sorted):\n            if i<len(out_sorted): mapping[c]=out_sorted[i]\n        mapping[bg_in]=bg_out\n    return mapping\n\ndef apply_color_map(arr,cmap):\n    out=arr.copy()\n    for ci,co in cmap.items(): out[arr==ci]=co\n    return out\n\n# --- CORE BASE OPS ---\ndef op_identity(arr,H,W,train_pairs=None): return pad_to(arr,H,W,bg=bg_color(arr))\ndef op_rotate90(arr,H,W,train_pairs=None): return pad_to(rotate(arr,1),H,W,bg=bg_color(arr))\ndef op_rotate180(arr,H,W,train_pairs=None): return pad_to(rotate(arr,2),H,W,bg=bg_color(arr))\ndef op_rotate270(arr,H,W,train_pairs=None): return pad_to(rotate(arr,3),H,W,bg=bg_color(arr))\ndef op_mirror_h(arr,H,W,train_pairs=None): return pad_to(mirror_h(arr),H,W,bg=bg_color(arr))\ndef op_mirror_v(arr,H,W,train_pairs=None): return pad_to(mirror_v(arr),H,W,bg=bg_color(arr))\ndef op_transpose(arr,H,W,train_pairs=None): return pad_to(transpose(arr),H,W,bg=bg_color(arr))\ndef op_crop_center_pad(arr,H,W,train_pairs=None):\n    return pad_to(crop_to_bbox(arr,bg=bg_color(arr)), H, W, bg=bg_color(arr), align='center')\ndef op_recolor_learned(arr,H,W,train_pairs=None):\n    cmap = learn_color_map(train_pairs or [])\n    recol = apply_color_map(arr,cmap)\n    return pad_to(recol,H,W,bg=bg_color(recol))\ndef op_keep_largest_cc_only(arr, H, W, train_pairs=None):\n    bg = bg_color(arr); a = pad_to(arr, H, W, bg=bg)\n    comps = connected_components(a, bg=bg)\n    if not comps: return a\n    _, pix = max(comps, key=lambda cp: len(cp[1]))\n    out = np.full_like(a, bg)\n    for (y,x) in pix: out[y,x] = a[y,x]\n    return out\ndef op_tile_smallest_patch(arr,H,W,train_pairs=None):\n    tile=smallest_tile(arr)\n    if tile is None: return pad_to(arr,H,W,bg=bg_color(arr))\n    th,tw=tile.shape\n    tiled=np.tile(tile,(max(1,math.ceil(H/th)), max(1,math.ceil(W/tw))))\n    return tiled[:H,:W]\n\n# --- ADVANCED STRUCTURAL/MANIPULATION OPS ---\ndef op_grow_largest_cc(arr, H, W, train_pairs=None):\n    bg = bg_color(arr); a = pad_to(arr, H, W, bg=bg)\n    comps = connected_components(a, bg=bg)\n    if not comps: return a\n    _, pix = max(comps, key=lambda cp: len(cp[1]))\n    mask = np.full_like(a, bg)\n    for y, x in pix: mask[y, x] = a[y, x]\n    return pad_to(dilate(mask, bg=bg, k=1), H, W, bg=bg)\ndef op_shrink_largest_cc(arr, H, W, train_pairs=None):\n    bg = bg_color(arr); a = pad_to(arr, H, W, bg=bg)\n    comps = connected_components(a, bg=bg)\n    if not comps: return a\n    _, pix = max(comps, key=lambda cp: len(cp[1]))\n    mask = np.full_like(a, bg)\n    for y, x in pix: mask[y, x] = a[y, x]\n    return pad_to(erode(mask, bg=bg, k=1), H, W, bg=bg)\ndef op_invert_colors(arr, H, W, train_pairs=None):\n    bg = bg_color(arr); a = pad_to(arr, H, W, bg=bg); out = a.copy()\n    vals = [c for c in np.unique(a) if c != bg]\n    if len(vals) >= 2:\n        counts = Counter(a.flatten())\n        sorted_vals = sorted(vals, key=lambda c: counts.get(c, 0), reverse=True)\n        c1, c2 = sorted_vals[0], sorted_vals[1]\n        out[a == c1] = c2\n        out[a == c2] = c1\n    return out\ndef op_normalize_color_to_1(arr, H, W, train_pairs=None):\n    bg = bg_color(arr); a = pad_to(arr, H, W, bg=bg); out = a.copy()\n    unique_non_bg = [c for c in np.unique(a) if c != bg]\n    if not unique_non_bg: return out\n    counts = Counter(a.flatten())\n    sorted_colors = sorted(unique_non_bg, key=lambda c: counts.get(c, 0), reverse=True)\n    new_color_map = {bg: bg}; target_color = 1\n    for old_color in sorted_colors:\n        if old_color == bg: continue\n        while target_color == bg: target_color = (target_color % 9) + 1\n        new_color_map[old_color] = target_color\n        target_color = (target_color % 9) + 1\n    for old_c, new_c in new_color_map.items():\n        out[a == old_c] = new_c\n    return out\ndef op_replace_cc_with_pixel(arr, H, W, train_pairs=None):\n    bg = bg_color(arr); a = pad_to(arr, H, W, bg=bg);\n    comps = connected_components(a, bg=bg);\n    out = np.full_like(a, bg);\n    for color, pix in comps:\n        if not pix: continue\n        y0, x0, y1, x1 = bbox_of_pixels(pix)\n        cy = (y0 + y1) // 2\n        cx = (x0 + x1) // 2\n        out[cy, cx] = color\n    return out\ndef op_complete_by_symmetry(arr, H, W, train_pairs=None):\n    bg = bg_color(arr); a = pad_to(arr, H, W, bg=bg); Hh, Ww = a.shape; out = a.copy()\n    L = a[:, :Ww//2]; R_candidate = a[:, Ww - (Ww//2):]\n    if Ww % 2 != 0: C = a[:, Ww//2]; R_candidate = a[:, Ww//2 + 1:]\n    if np.count_nonzero(L != bg) > 0.5 * Ww * Hh and np.count_nonzero(R_candidate != bg) < 0.1 * Ww * Hh:\n        mirror = mirror_h(L)\n        if Ww % 2 != 0: out[:, Ww//2 + 1:] = mirror\n        else: out[:, Ww//2:] = mirror\n        return out\n    T = a[:Hh//2, :]; B_candidate = a[Hh - (Hh//2):, :]\n    if np.count_nonzero(T != bg) > 0.5 * Ww * Hh and np.count_nonzero(B_candidate != bg) < 0.1 * Ww * Hh:\n        mirror = mirror_v(T)\n        if Hh % 2 != 0: out[Hh//2 + 1:, :] = mirror\n        else: out[Hh//2:, :] = mirror\n        return out\n    return a\ndef op_recolor_by_ratio(arr, H, W, train_pairs=None):\n    if not train_pairs: return pad_to(arr, H, W, bg=bg_color(arr))\n    input_sizes = [x.size for x, _ in train_pairs]\n    output_sizes = [y.size for _, y in train_pairs]\n    if not input_sizes or not output_sizes: return pad_to(arr, H, W, bg=bg_color(arr))\n    avg_ratio = np.mean(output_sizes) / np.mean(input_sizes)\n    if avg_ratio > 4.0: target_color = 3\n    elif avg_ratio > 1.5: target_color = 2\n    elif avg_ratio < 0.5: target_color = 1\n    else: return op_recolor_learned(arr, H, W, train_pairs)\n    bg = bg_color(arr); a = pad_to(arr, H, W, bg=bg); out = a.copy()\n    out[out != bg] = target_color\n    return out\ndef op_process_blocks(arr, H, W, train_pairs=None):\n    bg = bg_color(arr); a = pad_to(arr, H, W, bg=bg)\n    train_H = [x.shape[0] for x, _ in train_pairs]\n    train_W = [x.shape[1] for x, _ in train_pairs]\n    if len(train_H) < 2 or len(set(train_H)) < 2: bh, bw = 3, 3\n    else: bh = min(train_H); bw = min(train_W)\n    blocks = decompose_into_blocks(a, bh, bw)\n    if len(blocks) < 2: return a\n    major_color = majority_color(blocks[0])\n    out = np.full_like(a, major_color)\n    return out\ndef op_trim_to_non_background_grid(arr, H, W, train_pairs=None):\n    \"\"\"Crops the grid to only the non-background components, then pads back.\"\"\"\n    bg = bg_color(arr);\n    cropped = crop_to_bbox(arr, bg=bg)\n    # Apply a learned simple transform to the cropped part before padding back\n    if train_pairs and len(train_pairs) > 0:\n        if cropped.shape == train_pairs[0][0].shape:\n            # Try to rotate the cropped part if the IO has a rotation pattern\n            if all(np.array_equal(rotate(x, 1), y) for x, y in train_pairs[:2]):\n                cropped = rotate(cropped, 1)\n    return pad_to(cropped, H, W, bg=bg)\n\n# --- NEW SOLVER C: Recursive Object-Based Transformation ---\ndef op_recursive_cc_transformation(arr, H, W, train_pairs):\n    \"\"\"Transforms individual connected components (objects) and places them back.\"\"\"\n    bg = bg_color(arr); a = pad_to(arr, H, W, bg=bg)\n    comps = connected_components(a, bg=bg)\n    out = np.full((H, W), bg, dtype=int)\n\n    # Simple rule learning: Find the transformation applied to the LARGEST object\n    if not comps or not train_pairs: return a\n\n    # 1. Identify the rule from the first training pair\n    x_train, y_train = train_pairs[0]\n    x_comps = connected_components(x_train, bg=bg_color(x_train))\n    y_comps = connected_components(y_train, bg=bg_color(y_train))\n\n    if not x_comps or not y_comps: return a\n\n    # Find the largest component in the input\n    _, x_pix = max(x_comps, key=lambda cp: len(cp[1]))\n    x_y0, x_x0, x_y1, x_x1 = bbox_of_pixels(x_pix)\n    x_obj = crop_to_bbox(x_train[x_y0:x_y1+1, x_x0:x_x1+1], bg=bg_color(x_train))\n\n    # Trying to find the corresponding largest component in the output (by bounding box overlap/color)\n    best_match_y = None\n    max_overlap = -1\n    for y_color, y_pix in y_comps:\n        y_y0, y_x0, y_y1, y_x1 = bbox_of_pixels(y_pix)\n        if len(y_pix) > max_overlap:\n            max_overlap = len(y_pix)\n            best_match_y = y_train[y_y0:y_y1+1, y_x0:y_x1+1]\n\n    if best_match_y is None: return a\n\n    y_obj = crop_to_bbox(best_match_y, bg=bg_color(y_train))\n\n    # 2. Infer the transformation (simple check: rotate/mirror)\n    rule = op_identity\n    if x_obj.shape == y_obj.shape:\n        if hamming(rotate(x_obj, 1), y_obj) == 0: rule = op_rotate90\n        elif hamming(mirror_h(x_obj), y_obj) == 0: rule = op_mirror_h\n        elif hamming(mirror_v(x_obj), y_obj) == 0: rule = op_mirror_v\n\n    # 3. Apply the rule to all components in the test input\n    for color, pix in comps:\n        y0, x0, y1, x1 = bbox_of_pixels(pix)\n        component_grid = arr[y0:y1+1, x0:x1+1]\n        transformed_grid = rule(component_grid, component_grid.shape[0], component_grid.shape[1], None)\n        h, w = transformed_grid.shape\n        out[y0:y0+h, x0:x0+w] = transformed_grid\n\n    return pad_to(out, H, W, bg=bg)\n\n# Aggregate ALL operations\nBASIC_OPS = [\n    op_identity, op_rotate90, op_rotate180, op_rotate270,\n    op_mirror_h, op_mirror_v, op_transpose,\n    op_crop_center_pad, op_recolor_learned,\n    op_keep_largest_cc_only, op_tile_smallest_patch,\n    # Advanced Ops\n    op_grow_largest_cc, op_shrink_largest_cc,\n    op_invert_colors, op_normalize_color_to_1,\n    op_replace_cc_with_pixel,\n    # Ultimate Ops\n    op_complete_by_symmetry,\n    op_recolor_by_ratio,\n    op_process_blocks,\n    op_trim_to_non_background_grid,\n]\n\n# Cell 5: candidate generation, survivor selection, signature, ranking\n\n# --- SOLVER B: IO-to-IO Abstraction (non-CC solver) ---\ndef op_io_global_color_map(arr, H, W, train_pairs):\n    if not train_pairs: return pad_to(arr, H, W, bg=bg_color(arr))\n    all_mappings = defaultdict(lambda: Counter())\n    for x_in, y_out in train_pairs:\n        bg_in = bg_color(x_in); bg_out = bg_color(y_out)\n        x_fg_colors = [c for c in np.unique(x_in) if c != bg_in]\n        y_fg_colors = [c for c in np.unique(y_out) if c != bg_out]\n        in_counts = Counter(x_in.flatten())\n        out_counts = Counter(y_out.flatten())\n        in_ranked = sorted(x_fg_colors, key=lambda c: in_counts[c], reverse=True)\n        out_ranked = sorted(y_fg_colors, key=lambda c: out_counts[c], reverse=True)\n        for i, c_in in enumerate(in_ranked):\n            if i < len(out_ranked): all_mappings[c_in][out_ranked[i]] += 1\n        all_mappings[bg_in][bg_out] += 1\n    final_map = {}\n    for c_in, c_out_counter in all_mappings.items():\n        if c_out_counter: final_map[c_in] = c_out_counter.most_common(1)[0][0]\n    recol = apply_color_map(arr, final_map)\n    return pad_to(recol, H, W, bg=bg_color(recol))\n\n# --- GENERATE CANDIDATES (Now includes Solver C) ---\ndef generate_candidates(x_in, H, W, train_pairs, use_compositions=True):\n    cands=[]\n\n    # Solver A: Heuristic Compositions\n    for f in BASIC_OPS:\n        try:\n            y=f(x_in,H,W,train_pairs)\n            if y.shape==(H,W): cands.append((f\"op:{f.__name__}\", y))\n        except Exception:\n            pass\n    if use_compositions:\n        for f,g in itertools.product(BASIC_OPS, BASIC_OPS):\n            try:\n                y1=g(x_in,H,W,train_pairs)\n                if y1.shape!=(H,W): continue\n                y2=f(y1,H,W,train_pairs)\n                if y2.shape==(H,W): cands.append((f\"op2:{f.__name__}+{g.__name__}\", y2))\n            except Exception:\n                pass\n\n    # Solver B: IO-to-IO Abstraction\n    try:\n        y_io = op_io_global_color_map(x_in, H, W, train_pairs)\n        if y_io.shape==(H,W): cands.append((\"io_abstraction:op_io_global_color_map\", y_io))\n    except Exception:\n        pass\n\n    # Solver C: Recursive Object Transformation\n    try:\n        y_cc = op_recursive_cc_transformation(x_in, H, W, train_pairs)\n        if y_cc.shape==(H,W): cands.append((\"object_manip:op_recursive_cc_transformation\", y_cc))\n    except Exception:\n        pass\n\n    # Deduplicate candidates\n    uniq={}\n    for lbl,arr in cands:\n        key=arr.tobytes()\n        if key not in uniq: uniq[key]=(lbl,arr)\n    return list(uniq.values())\n\n# --- UTILITIES ---\ndef infer_target_shape(train_pairs):\n    Hs=[y.shape[0] for _,y in train_pairs]\n    Ws=[y.shape[1] for _,y in train_pairs]\n    H=Counter(Hs).most_common(1)[0][0]\n    W=Counter(Ws).most_common(1)[0][0]\n    return H,W\n\n# NEW: Input-aware target shape inference (per test input)\ndef infer_target_shape_for_input(train_pairs, x_test):\n    \"\"\"Infer the correct (H,W) for this specific test input.\"\"\"\n    Hi, Wi = x_test.shape\n\n    # Collect additive and multiplicative shape relationships from train\n    add_rules = []\n    mul_rules = []\n    for x, y in train_pairs:\n        hi, wi = x.shape\n        ho, wo = y.shape\n        add_rules.append((ho - hi, wo - wi))\n        # only keep integer scale rules\n        sh = ho / max(1, hi)\n        sw = wo / max(1, wi)\n        if abs(sh - round(sh)) < 1e-9 and abs(sw - round(sw)) < 1e-9:\n            mul_rules.append((int(round(sh)), int(round(sw))))\n\n    # 1) If *all* train pairs keep same size, use the test input size\n    if all(dh == 0 and dw == 0 for dh, dw in add_rules):\n        return Hi, Wi\n\n    # 2) If all additive deltas are identical, apply that delta\n    if len(set(add_rules)) == 1:\n        dh, dw = add_rules[0]\n        return max(1, Hi + dh), max(1, Wi + dw)\n\n    # 3) If all integer scales are identical, apply that scale\n    if mul_rules and len(set(mul_rules)) == 1:\n        sh, sw = mul_rules[0]\n        return max(1, Hi * sh), max(1, Wi * sw)\n\n    # 4) Majority heuristic\n    from collections import Counter as _Counter\n    add_major = _Counter(add_rules).most_common(1)[0]\n    mul_major = _Counter(mul_rules).most_common(1)[0] if mul_rules else None\n    if add_major[1] >= max(2, len(train_pairs)//2):\n        dh, dw = add_major[0]\n        return max(1, Hi + dh), max(1, Wi + dw)\n    if mul_major and mul_major[1] >= max(2, len(train_pairs)//2):\n        sh, sw = mul_major[0]\n        return max(1, Hi * sh), max(1, Wi * sw)\n\n    # 5) Fallback: keep test input size (safest default)\n    return Hi, Wi\n\ndef grid_features(a):\n    bg = bg_color(a)\n    colors = len(np.unique(a))\n    ccs = len(connected_components(a, bg=bg))\n    filled = int(np.count_nonzero(a != bg))\n    return {\"colors\": colors, \"ccs\": ccs, \"filled\": filled}\ndef learn_output_signature(train_pairs):\n    feats = [grid_features(y) for _, y in train_pairs]\n    if not feats:\n        return {\"colors\": None, \"ccs\": None, \"filled\": None, \"best_grid\": None}\n    colors_med = int(np.median([f[\"colors\"] for f in feats]))\n    ccs_med    = int(np.median([f[\"ccs\"]    for f in feats]))\n    filled_med = int(np.median([f[\"filled\"] for f in feats]))\n    output_grids = [y for _, y in train_pairs]\n    output_key = Counter(y.tobytes() for y in output_grids).most_common(1)\n    best_grid = None\n    if output_key:\n        best_bytes = output_key[0][0]\n        for y in output_grids:\n            if y.tobytes() == best_bytes:\n                best_grid = y.tolist()\n                break\n    return {\"colors\": colors_med, \"ccs\": ccs_med, \"filled\": filled_med, \"best_grid\": best_grid}\ndef filter_by_signature(cands, sig, tol_colors=1, tol_ccs=1, tol_filled_ratio=0.6):\n    if not sig or sig[\"colors\"] is None: return cands\n    kept = []\n    for lbl, y in cands:\n        f = grid_features(y)\n        ok = True\n        if abs(f[\"colors\"] - sig[\"colors\"]) > tol_colors: ok = False\n        if abs(f[\"ccs\"]    - sig[\"ccs\"])    > tol_ccs:    ok = False\n        if sig[\"filled\"] > 0:\n            if abs(f[\"filled\"] - sig[\"filled\"]) > max(3, int(sig[\"filled\"] * tol_filled_ratio)): ok = False\n        if ok: kept.append((lbl, y))\n    return kept if kept else cands\n\n# --- SURVIVOR SELECTION (maps ALL three solvers) ---\ndef select_survivor_labels(train_pairs, H, W, use_compositions, time_limit=2.0):\n    t0=time.time()\n    x0,y0=train_pairs[0]\n    pool0=generate_candidates(x0,H,W,train_pairs,use_compositions=use_compositions)\n\n    program_map = {p.__name__: p for p in BASIC_OPS}\n    program_map['op_io_global_color_map'] = op_io_global_color_map\n    program_map['op_recursive_cc_transformation'] = op_recursive_cc_transformation  # Map Solver C\n\n    program_scores = defaultdict(lambda: {'matches': 0, 'weighted_score': -1e9})\n    labels = [lbl for lbl,_ in pool0]\n\n    for lbl in labels:\n        if time.time()-t0 > time_limit: break\n\n        parts = lbl.split(':')[-1].split('+')\n        f_name = parts[0]; g_name = parts[1] if len(parts) > 1 else None\n\n        is_solver_b = (f_name == 'op_io_global_color_map')\n        is_solver_c = (f_name == 'op_recursive_cc_transformation')\n\n        if is_solver_b: f = op_io_global_color_map; g = None\n        elif is_solver_c: f = op_recursive_cc_transformation; g = None\n        else: f = program_map.get(f_name); g = program_map.get(g_name) if g_name else None\n\n        if f is None or (g_name and g is None and g_name != \"None\"): continue\n\n        current_matches = 0; current_score = 0.0\n\n        for x, y_true in train_pairs:\n            try:\n                if is_solver_b: y_cand = op_io_global_color_map(x, H, W, train_pairs)\n                elif is_solver_c: y_cand = op_recursive_cc_transformation(x, H, W, train_pairs)\n                else: y_cand = f(g(x,H,W,train_pairs), H, W, train_pairs) if g else f(x,H,W,train_pairs)\n\n                if y_cand.shape != y_true.shape: current_score -= 100.0\n                else:\n                    dist = hamming(y_cand, y_true)\n                    if dist == 0:\n                        current_score += 1000.0\n                        current_matches += 1\n                    else: current_score -= dist * 0.01\n            except Exception: current_score -= 500.0\n\n        program_scores[lbl]['matches'] = current_matches\n        program_scores[lbl]['weighted_score'] = current_score\n\n    max_matches = 0\n    if program_scores: max_matches = max(s['matches'] for s in program_scores.values())\n\n    survivors = []\n    if max_matches > 0:\n        top_performers = sorted(\n            [(lbl, s['weighted_score']) for lbl, s in program_scores.items() if s['matches'] == max_matches],\n            key=lambda x: x[1], reverse=True\n        )\n        survivors = [lbl for lbl, _ in top_performers[:10]]\n\n    if not survivors and program_scores:\n        fallbacks = sorted(program_scores.items(), key=lambda kv: kv[1]['weighted_score'], reverse=True)\n        survivors = [l for l, _ in fallbacks[:3]]\n\n    return survivors\n\n# --- ULTIMATE RANKING: Op-Type Diversity Enforcement (handles 3 solver types) ---\ndef rank_candidates_for_test_top_N(cand_pool, survivors, sig=None, N=2):\n    scored_candidates = []\n\n    for lbl, y in cand_pool:\n        score = 0.0\n        if lbl not in survivors: score += 500.0\n        else: score -= 50.0\n\n        if sig and sig[\"colors\"] is not None:\n            f = grid_features(y)\n            score += abs(f[\"colors\"] - sig[\"colors\"]) * 1.5\n            score += abs(f[\"ccs\"]    - sig[\"ccs\"])    * 1.0\n            if sig[\"filled\"] > 0: score += abs(f[\"filled\"] - sig[\"filled\"]) / max(1, sig[\"filled\"]) * 2.0\n\n        scored_candidates.append({'score': score, 'pred_arr': y, 'lbl': lbl})\n\n    scored_candidates.sort(key=lambda x: x['score'])\n\n    unique_preds = []\n    seen_keys = set()\n    seen_categories = set()\n\n    if scored_candidates:\n        pred_1 = scored_candidates[0]\n        pred_arr_1 = pred_1['pred_arr']\n        unique_preds.append(pred_arr_1)\n        seen_keys.add(pred_arr_1.tobytes())\n        seen_categories.add(get_op_category(pred_1['lbl']))\n\n    if N == 2 and len(unique_preds) == 1:\n        H, W = pred_arr_1.shape\n        pred_arr_2 = None\n        DIVERSITY_THRESHOLD = max(5, int(0.08 * H * W))\n\n        for cand in scored_candidates:\n            current_arr = cand['pred_arr']\n            key = current_arr.tobytes()\n            op_cat = get_op_category(cand['lbl'])\n\n            if key in seen_keys: continue\n\n            is_structurally_diverse = (current_arr.shape == pred_arr_1.shape and hamming(current_arr, pred_arr_1) > DIVERSITY_THRESHOLD)\n            is_functionally_diverse = (op_cat not in seen_categories)\n\n            if is_structurally_diverse and is_functionally_diverse:\n                pred_arr_2 = current_arr\n                unique_preds.append(pred_arr_2)\n                break\n\n        if pred_arr_2 is None:\n            for cand in scored_candidates:\n                current_arr = cand['pred_arr']\n                key = current_arr.tobytes()\n                if key not in seen_keys and current_arr.shape == pred_arr_1.shape and hamming(current_arr, pred_arr_1) > DIVERSITY_THRESHOLD:\n                    pred_arr_2 = current_arr\n                    unique_preds.append(pred_arr_2)\n                    break\n\n        if pred_arr_2 is None and sig and sig.get(\"best_grid\"):\n            pred_arr_2 = to_np(sig[\"best_grid\"])\n            if pred_arr_2.shape != (H, W): pred_arr_2 = pad_to(pred_arr_2, H, W)\n\n            if hamming(pred_arr_2, pred_arr_1) < DIVERSITY_THRESHOLD:\n                pred_arr_2 = op_rotate90(pred_arr_1, H, W)\n\n            if pred_arr_2.tobytes() not in seen_keys:\n                unique_preds.append(pred_arr_2)\n            else:\n                unique_preds.append(op_mirror_h(pred_arr_1, H, W))\n\n    while len(unique_preds) < N:\n        unique_preds.append(unique_preds[0])\n\n    return unique_preds[:N]\n\n# Cell 6: SOLVER + I/O (Time-Optimized Orchestration) — UPDATED PER-INPUT SHAPE + IDENTITY BACKSTOP\ndef solve_task_arc_prize(task, time_limit=MAX_TIME_PER_TASK):\n    t0=time.time()\n    train_pairs=[(to_np(p[\"input\"]), to_np(p[\"output\"])) for p in task[\"train\"]]\n    test_inputs=[to_np(p[\"input\"]) for p in task[\"test\"]]\n\n    # Keep global H,W for survivor selection only\n    H_sel,W_sel = infer_target_shape(train_pairs)\n\n    # Allocate time to survivor search for stability\n    survivors = select_survivor_labels(train_pairs, H_sel, W_sel, USE_COMPOSITIONS, time_limit=max(1.5, time_limit*0.75))\n\n    sig = learn_output_signature(train_pairs)\n\n    predictions = []\n    for x in test_inputs:\n        # Infer the correct target shape for THIS test input\n        Hx, Wx = infer_target_shape_for_input(train_pairs, x)\n\n        time_left = time_limit - (time.time() - t0)\n        # Minimum time check for test case execution\n        if time_left < 0.05:\n            default_pred = pad_to(x, Hx, Wx)\n            identity_pred = pad_to(x, x.shape[0], x.shape[1])\n            predictions.append([{\"output\": to_list(default_pred)}, {\"output\": to_list(identity_pred)}])\n            continue\n\n        # Generate candidates from ALL three solvers (A, B, C) with per-input target size\n        cand_pool = generate_candidates(x, Hx, Wx, train_pairs, use_compositions=USE_COMPOSITIONS)\n        filtered_cand_pool = filter_by_signature(cand_pool, sig, tol_colors=1, tol_ccs=1, tol_filled_ratio=0.6)\n\n        # Rank the combined, filtered candidates with Op-Type Diversity\n        top_2_preds = rank_candidates_for_test_top_N(filtered_cand_pool, survivors, sig=sig, N=2)\n\n        # Identity backstop at exact input size\n        identity_pred = pad_to(x, x.shape[0], x.shape[1])\n\n        pred_1_arr = top_2_preds[0] if len(top_2_preds) >= 1 else pad_to(x, Hx, Wx)\n        pred_2_arr = top_2_preds[1] if len(top_2_preds) >= 2 else identity_pred\n\n        # If pred_2 equals pred_1, force identity fallback to guarantee diversity\n        if pred_2_arr.shape == pred_1_arr.shape and np.array_equal(pred_2_arr, pred_1_arr):\n            pred_2_arr = identity_pred\n\n        predictions.append([\n            {\"output\": to_list(pred_1_arr)},\n            {\"output\": to_list(pred_2_arr)}\n        ])\n\n    return predictions\n\ndef solve_many(tasks):\n    outs=[]\n    for i,task_data in enumerate(tasks):\n        if (i % PRINT_PROGRESS_EVERY)==0:\n            print(f\"Solving task {i+1}/{len(tasks)}\")\n        try:\n            preds=solve_task_arc_prize(task_data, time_limit=MAX_TIME_PER_TASK)\n        except Exception:\n            # Fallback to a single default prediction in case of exception during solve\n            preds=[[{\"output\": [[0]]}, {\"output\": [[0]]}]] * len(task_data.get(\"test\", []))\n        outs.append(preds)\n    return outs\n\n# ----------------------------------------------------------------\n# Local Evaluation Function\n# ----------------------------------------------------------------\ndef evaluate_on_training(tasks):\n    \"\"\"\n    Evaluates local performance by scoring the number of correct OUTPUTS\n    (puzzles solved) using BOTH attempts, mimicking the competition.\n    \"\"\"\n    total_outputs = 0\n    correct_outputs = 0\n\n    print(\"\\n--- Running New, Corrected Local Proxy Accuracy Check ---\")\n\n    for i, task in enumerate(tasks):\n        fake_task={\"train\": task[\"train\"], \"test\": [{\"input\": t[\"input\"]} for t in task[\"train\"]]}\n        preds_list_of_lists = solve_task_arc_prize(fake_task, time_limit=MAX_TIME_PER_TASK * 1.5)\n        y_true_list=[to_np(t[\"output\"]) for t in task[\"train\"]]\n\n        for j, y_true in enumerate(y_true_list):\n            total_outputs += 1\n            if j >= len(preds_list_of_lists): continue\n            pred_list = preds_list_of_lists[j]\n\n            if len(pred_list) < 2 or \"output\" not in pred_list[0] or \"output\" not in pred_list[1]:\n                 continue\n\n            try:\n                 p1 = to_np(pred_list[0][\"output\"])\n                 p2 = to_np(pred_list[1][\"output\"])\n\n                 is_p1_correct = (p1.shape == y_true.shape and hamming(p1, y_true) == 0)\n                 is_p2_correct = (p2.shape == y_true.shape and hamming(p2, y_true) == 0)\n\n                 if is_p1_correct or is_p2_correct:\n                     correct_outputs += 1\n            except Exception:\n                 continue  # Ignore tasks that fail numpy conversion\n\n    accuracy = correct_outputs / total_outputs if total_outputs > 0 else 0\n    return accuracy, correct_outputs, total_outputs\n\n# Cell 7: Data Loading & Local Evaluation\ndef load_keyed_arc_tasks(path):\n    \"\"\"Loads ARC tasks from path, ensuring they are keyed by their IDs.\"\"\"\n    with open(path, \"r\") as f:\n        raw = f.read().strip()\n\n    if not (raw.startswith(\"{\") or raw.startswith(\"[\")):\n        tasks_list = load_arc_tasks_anyshape(path)\n        return {f\"task_{i:03d}\": t for i, t in enumerate(tasks_list)}\n\n    obj = json.loads(raw)\n\n    if isinstance(obj, dict):\n        return {k: v for k, v in obj.items() if 'train' in v or 'test' in v}\n\n    tasks_list = load_arc_tasks_anyshape(path)\n    return {f\"task_{i:03d}\": t for i, t in enumerate(tasks_list)}\n\n# 1. Load training data as a list for the local evaluation\ntrain_tasks = load_arc_tasks_anyshape(TRAIN_JSON)\nprint(\"Training tasks (normalized):\", len(train_tasks))\n\n# 2. CRITICAL: Load test data as a DICTIONARY to preserve the Task IDs\ntest_tasks_dict = load_keyed_arc_tasks(TEST_JSON)\nprint(\"Test tasks (normalized):\", len(test_tasks_dict))\ntest_tasks_list = list(test_tasks_dict.values())\ntest_task_ids = list(test_tasks_dict.keys())\n\n# Sanity check\nif train_tasks:\n    print(f\"<class 'dict'> {train_tasks[0].keys()}\")\n\n# 3. Proxy evaluation\nlocal_accuracy, solved_outputs, total_outputs = evaluate_on_training(train_tasks)\nprint(\"\\n--- Local Accuracy Report (FINAL RECURSIVE MULTI-MODAL ENGINE) ---\")\nprint(f\"Total training outputs (puzzles) checked: {total_outputs}\")\nprint(f\"Correctly solved outputs (by either attempt): {solved_outputs}\")\nprint(f\"**Local Proxy Accuracy:** {local_accuracy:.4f} (Goal: 0.8500+)\")  # Ignore this score!\n\n# 4. Run the full solver on the test set\nprint(\"\\n--- Running Full Test Solver ---\")\ntest_predictions = solve_many(test_tasks_list)\nprint(\"Full test solver complete.\")\n\n# Cell 8: Submission Writer\ndef write_submission(task_ids, predictions, path):\n    \"\"\"\n    Formats the predictions into the required Kaggle submission JSON structure.\n    \"\"\"\n    submission_dict = {}\n\n    for task_id, task_preds in zip(task_ids, predictions):\n\n        # NOTE: The predictions format from solve_many is:\n        # [[{'output': grid1}, {'output': grid2}], [{'output': grid1}, {'output': grid2}], ...]\n\n        formatted_preds = []\n        for pred_pair in task_preds:\n\n            # Ensure the structure is right, default if not\n            a1 = pred_pair[0].get(\"output\", [[0]])\n            a2 = pred_pair[1].get(\"output\", [[0]])\n\n            formatted_preds.append({\n                \"attempt_1\": a1,\n                \"attempt_2\": a2,\n            })\n\n        submission_dict[task_id] = formatted_preds\n\n    with open(path, \"w\") as f:\n        json.dump(submission_dict, f)\n\n    # Final structure check\n    if all(len(v)==len(test_tasks_dict[k][\"test\"]) for k,v in submission_dict.items()):\n          print(\"Submission structure OK ✅ — Go ahead and Submit to Competition.\")\n    else:\n          print(\"WARNING: Submission structure mismatch detected.\")\n\n# Write the final submission file\nwrite_submission(test_task_ids, test_predictions, SUBMISSION_PATH)\n\nprint(f\"Wrote submission to: {SUBMISSION_PATH}\")\n\n# Cell 9: Final Submission Sanity Check\nprint(\"--- Running FINAL Submission File Sanity Check ---\")\n\ntry:\n    with open(SUBMISSION_PATH, 'r') as f:\n        final_submission = json.load(f)\n\n    task_ids = list(final_submission.keys())\n\n    # 1. Check total tasks\n    print(f\"\\n✅ Total tasks found in submission: {len(task_ids)}\")\n    print(f\"   (Expected: 240, matching the test set)\")\n\n    # 2. Check a random sample task structure\n    if task_ids:\n        sample_task_id = task_ids[0]\n        sample_preds = final_submission[sample_task_id]\n\n        print(f\"\\n✅ Sample Task ID Checked: {sample_task_id}\")\n\n        # Check overall prediction count\n        expected_test_cases = len(test_tasks_dict[sample_task_id][\"test\"])\n        print(f\"   - Total test cases predicted: {len(sample_preds)}\")\n        print(f\"   - Expected test cases (from data): {expected_test_cases}\")\n\n        # Check the structure of the FIRST prediction in the list\n        if sample_preds and isinstance(sample_preds, list):\n            first_pred = sample_preds[0]\n\n            # Check for required keys (attempt_1, attempt_2)\n            required_keys = [\"attempt_1\", \"attempt_2\"]\n            if all(k in first_pred for k in required_keys):\n                print(f\"   - Prediction keys are correct: {list(first_pred.keys())}\")\n\n                # Check the content type (should be a list of lists)\n                a1 = first_pred['attempt_1']\n                a2 = first_pred['attempt_2']\n\n                is_a1_grid = isinstance(a1, list) and (not a1 or isinstance(a1[0], list))\n                is_a2_grid = isinstance(a2, list) and (not a2 or isinstance(a2[0], list))\n\n                if is_a1_grid and is_a2_grid:\n                    print(\"   - Attempt content is correctly formatted as List[List[int]]\")\n                    print(\"\\n--- Sample Prediction Content (First Test Case) ---\")\n                    print(f\"Task ID: {sample_task_id}\")\n                    # Safety check for empty lists\n                    a1_h = len(a1); a1_w = len(a1[0]) if a1 and a1[0] else 0\n                    a2_h = len(a2); a2_w = len(a2[0]) if a2 and a2[0] else 0\n                    print(f\"Attempt 1 Grid Shape: {a1_h}x{a1_w}\")\n                    print(f\"Attempt 2 Grid Shape: {a2_h}x{a2_w}\")\n                    # Print only the first 2 rows for brevity\n                    print(f\"Attempt 1 (Snippet): {a1[:2]}\")\n                    print(f\"Attempt 2 (Snippet): {a2[:2]}\")\n                    print(\"-----------------------------------------------------\")\n                else:\n                    print(\"❌ ERROR: Attempt content is NOT a list of lists (the grid format).\")\n\n            else:\n                print(f\"❌ ERROR: Missing required keys 'attempt_1' or 'attempt_2'. Keys found: {list(first_pred.keys())}\")\n        else:\n            print(\"❌ ERROR: Predictions for the task are not in a List format.\")\n\nexcept FileNotFoundError:\n    print(f\"❌ ERROR: Submission file not found at {SUBMISSION_PATH}. Did Cell 8 run successfully?\")\nexcept json.JSONDecodeError:\n    print(\"❌ ERROR: Submission file is not valid JSON.\")\nexcept Exception as e:\n    print(f\"❌ An unexpected error occurred during check: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T02:07:35.822337Z","iopub.execute_input":"2025-10-16T02:07:35.822642Z","iopub.status.idle":"2025-10-16T02:18:42.219124Z","shell.execute_reply.started":"2025-10-16T02:07:35.822619Z","shell.execute_reply":"2025-10-16T02:18:42.217949Z"}},"outputs":[{"name":"stdout","text":"Training tasks (normalized): 1000\nTest tasks (normalized): 240\n<class 'dict'> dict_keys(['train', 'test'])\n\n--- Running New, Corrected Local Proxy Accuracy Check ---\n\n--- Local Accuracy Report (FINAL RECURSIVE MULTI-MODAL ENGINE) ---\nTotal training outputs (puzzles) checked: 3232\nCorrectly solved outputs (by either attempt): 155\n**Local Proxy Accuracy:** 0.0480 (Goal: 0.8500+)\n\n--- Running Full Test Solver ---\nSolving task 1/240\nSolving task 51/240\nSolving task 101/240\nSolving task 151/240\nSolving task 201/240\nFull test solver complete.\nSubmission structure OK ✅ — Go ahead and Submit to Competition.\nWrote submission to: /kaggle/working/submission.json\n--- Running FINAL Submission File Sanity Check ---\n\n✅ Total tasks found in submission: 240\n   (Expected: 240, matching the test set)\n\n✅ Sample Task ID Checked: 00576224\n   - Total test cases predicted: 1\n   - Expected test cases (from data): 1\n   - Prediction keys are correct: ['attempt_1', 'attempt_2']\n   - Attempt content is correctly formatted as List[List[int]]\n\n--- Sample Prediction Content (First Test Case) ---\nTask ID: 00576224\nAttempt 1 Grid Shape: 6x6\nAttempt 2 Grid Shape: 6x6\nAttempt 1 (Snippet): [[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8]]\nAttempt 2 (Snippet): [[3, 7, 3, 7, 3, 7], [2, 8, 2, 8, 2, 8]]\n-----------------------------------------------------\n","output_type":"stream"}],"execution_count":1}]}